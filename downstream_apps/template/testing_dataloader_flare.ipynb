{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467803a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import sunpy.visualization.colormaps as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Append base path.  May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to the wokshop_infrastructure folder.\n",
    "sys.path.append(\"../../\")\n",
    " \n",
    "# Append Surya path. May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to surya's release code.\n",
    "sys.path.append(\"../../Surya\")\n",
    "\n",
    "from surya.utils.data import build_scalers  # Data scaling utilities for Surya stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paths - modify these if your files are in different locations\n",
    "config_path = \"./configs/config.yaml\"\n",
    "\n",
    "# Load configuration\n",
    "print(\"ðŸ“‹ Loading configuration...\")\n",
    "try:\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
    "    print(\"Configuration loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure config.yaml exists in your current directory\")\n",
    "    raise\n",
    "\n",
    "scalers = build_scalers(info=config[\"data\"][\"scalers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92464e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SolarSuryaFlareDataset' from 'downstream_apps.template.datasets.dataset_flare' (/home/chetrajpandey/surya_workshop/downstream_apps/template/../../downstream_apps/template/datasets/dataset_flare.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdownstream_apps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemplate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_flare\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SolarSuryaFlareDataset\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SolarSuryaFlareDataset' from 'downstream_apps.template.datasets.dataset_flare' (/home/chetrajpandey/surya_workshop/downstream_apps/template/../../downstream_apps/template/datasets/dataset_flare.py)"
     ]
    }
   ],
   "source": [
    "from downstream_apps.template.datasets.dataset_flare import SolarSuryaFlareDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8401c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5b70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4bfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a842a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SolarFlareDataset(\n",
    "    #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "    index_path=config[\"data\"][\"train_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probablity\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    s3_use_simplecache = True,\n",
    "    s3_cache_dir= \"/tmp/helio_s3_cache\",    \n",
    "    #### Put your donwnstream (DS) specific parameters below this line\n",
    "    return_surya_stack=False,\n",
    "    max_number_of_samples=6,\n",
    "    flare_index_path=\"./data/validation.csv\",\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5043a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surya_ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
