{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d651fb69",
   "metadata": {},
   "source": [
    "# PyTorch Lightning baseline template\n",
    "\n",
    "by AndrÃ©s MuÃ±oz-Jaramillo\n",
    "\n",
    "This notebook is meant to act as a template to train and use a simple regression model to define a baseline that can be compared with a DS application.\n",
    "\n",
    "It focuses on the concept of defining a PyTorch model, a PyTorch lightning training loop and the deffinition of metrics of performance.\n",
    "\n",
    "This notebook assumes familiarity with the concepts of datasets and dataloaders contained in the **_0_dataset_dataloader_template.ipynb_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bade9c",
   "metadata": {},
   "source": [
    "## Set your cuda visible device\n",
    "\n",
    "**IMPORTANT:** Since we are sharing resources, please make sure that the cuda visible device you put here is the one assigned to your team and your machine.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d6af87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080614c",
   "metadata": {},
   "source": [
    "Here we initalize variables related to Weights and Biases, our online logging system to ensure they are user specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ca708db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure wandb logs are stored in a user-specific directory\n",
    "# Set writable directories\n",
    "os.environ[\"WANDB_DIR\"] = \"./wandb/wandb_logs\"\n",
    "os.environ[\"WANDB_CACHE_DIR\"] = \"./wandb/wandb_cache\"\n",
    "os.environ[\"WANDB_CONFIG_DIR\"] = \"./wandb/wandb_config\"\n",
    "# Optional:\n",
    "os.environ[\"TMPDIR\"] = \"./wandb/wandb_tmp\"\n",
    "\n",
    "# Ensure directories exist (optional, wandb usually creates them)\n",
    "os.makedirs(os.environ[\"WANDB_DIR\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"WANDB_CACHE_DIR\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"WANDB_CONFIG_DIR\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TMPDIR\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ed04321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger, WandbLogger\n",
    "\n",
    "# Append base path.  May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to the wokshop_infrastructure folder.\n",
    "sys.path.append(\"../../\")\n",
    " \n",
    "# Append Surya path. May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to surya's release code.\n",
    "sys.path.append(\"../../Surya\")\n",
    "\n",
    "from surya.utils.data import build_scalers  # Data scaling utilities for Surya stacks\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e9110",
   "metadata": {},
   "source": [
    "## Download scalers\n",
    "Surya input data needs to be scaled properly for the model to work and this cell downloads the scaling information.\n",
    "\n",
    "- If the cell below fails, try running the provided shell script directly in the terminal.\n",
    "- Sometimes the download may fail due to network or server issuesâ€”if that happens, simply re-run the script a few times until it completes successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08b87776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Checking assets directory at: /home/shahbahauddin/surya_workshop/downstream_apps/template/assets\n",
      "==> Downloading scalers.yaml from nasa-ibm-ai4science/core-sdo\n",
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 4275.54it/s]\n",
      "/home/shahbahauddin/surya_workshop/downstream_apps/template/assets\n",
      "âœ“ Done. File is located at: /home/shahbahauddin/surya_workshop/downstream_apps/template/assets/scalers.yaml\n"
     ]
    }
   ],
   "source": [
    "!sh download_scalers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04135ff7",
   "metadata": {},
   "source": [
    "## Load configuration\n",
    "\n",
    "Surya was designed to read a configuration file that defines many aspects of the model\n",
    "including the data it uses we use this config file to set default values that do not\n",
    "need to be modified, but also to define values specific to our downstream application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63c1fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Loading configuration...\n",
      "âœ… Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration paths - modify these if your files are in different locations\n",
    "config_path = \"./configs/config.yaml\"\n",
    "\n",
    "# Load configuration\n",
    "print(\"ðŸ“‹ Loading configuration...\")\n",
    "try:\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
    "    print(\"âœ… Configuration loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Make sure config.yaml exists in your current directory\")\n",
    "    raise\n",
    "\n",
    "scalers = build_scalers(info=config[\"data\"][\"scalers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6574dc",
   "metadata": {},
   "source": [
    "## Define Downstream (DS) datasets\n",
    "\n",
    "This child class takes as input all expected HelioFM parameters, plus additonal parameters relevant to the downstream application.  Here we focus in particular to the DS index and parameters necessary to combine it with the HelioFM index.\n",
    "\n",
    "Another important component of creating a dataset class for your DS is normalization.  Here we use a log normalization on xray flux that will act as the output target.  Making log10(xray_flux) strictly positive and having 66% of its values between 0 and 1\n",
    "\n",
    "In this case we will define both a training and a validation dataset using the indices pointed at in the config\n",
    "\n",
    "**_Important:  In this notebook we sets max_number_of_samples=6 to potentially avoid going through the whole dataset as we explore it.  Keep in mind this for the future in case the database seems smaller than you expect_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34db2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.datasets.template_dataset import FlareDSDataset\n",
    "from downstream_apps.template.datasets.eve_dataset import EVESpectraDSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70e9c5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to your downstream spectra CSV\n",
    "ds_spectra_csv_path = \"./euv_data/sample_6k_eve_spectra.csv\"\n",
    "\n",
    "# Train dataset (EVE)\n",
    "train_dataset = EVESpectraDSDataset(\n",
    "    #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "    # (You stated these must match the template exactly)\n",
    "    index_path=config[\"data\"][\"train_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probability\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    s3_use_simplecache=True,\n",
    "    s3_cache_dir=\"/tmp/helio_s3_cache\",\n",
    "    #### Put your downstream (DS) specific parameters below this line\n",
    "    return_surya_stack=True,\n",
    "    max_number_of_samples=64,        # keep small while debugging; increase later\n",
    "    ds_spectra_csv_path=ds_spectra_csv_path,\n",
    "    ds_time_column=\"timestamp\",\n",
    "    ds_time_tolerance=\"100d\",\n",
    "    ds_match_direction=\"forward\",\n",
    ")\n",
    "\n",
    "# Validation dataset (EVE) - follow template pattern\n",
    "val_dataset = EVESpectraDSDataset(\n",
    "    index_path=config[\"data\"][\"valid_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probability\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"val\",\n",
    "    s3_use_simplecache=True,\n",
    "    s3_cache_dir=\"/tmp/helio_s3_cache\",\n",
    "    return_surya_stack=True,\n",
    "    max_number_of_samples=64,\n",
    "    ds_spectra_csv_path=ds_spectra_csv_path,\n",
    "    ds_time_column=\"timestamp\",\n",
    "    ds_time_tolerance=\"100d\",\n",
    "    ds_match_direction=\"forward\",\n",
    ")\n",
    "\n",
    "len(train_dataset), len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fda7cf",
   "metadata": {},
   "source": [
    "We also intialize separate training and validation dataloaders.   Since we are working in a shared environment.  Using multiprocessing_context=\"spawn\" helps avoid lockups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48bb2912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ts', 'time_delta_input', 'forecast', 'lead_time_delta', 'ds_index'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                multiprocessing_context=\"spawn\",\n",
    "                persistent_workers=True,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "                dataset=val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=4,\n",
    "                multiprocessing_context=\"spawn\",\n",
    "                persistent_workers=True,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "batch = next(iter(train_data_loader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d2dea",
   "metadata": {},
   "source": [
    "## Define simple baseline model\n",
    "\n",
    "Defining a simple baseline is important to understand what value is bringing the AI model to the problem.  \n",
    "\n",
    "It is always very good to have a very simple baseline model.  Ideally one that cannot overfit the data.  This is a very good way of really measuring the value added of complex models.   Classical machine learning excels here:\n",
    "\n",
    "- Regressions and logistic regressions.\n",
    "- Climatological averages.\n",
    "- Persistance.\n",
    "- Simple transformations.\n",
    "\n",
    "Simple models avoid excesively optimistic assessments of the capatiblities of a complex models and for many problems are actually remarkably hard to beat.\n",
    "\n",
    "In this example we define a simple regression acting on the intensity of each channel.  Note that we invert the normalization to deal with strictly positive quantities.  As with the dataset we will be importing the model from a module so that we can use it within training scripts later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f5fd639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.models.simple_baseline import RegressionFlareModel\n",
    "from downstream_apps.template.models.eve_baseline import RegressionEVEModel\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d87992",
   "metadata": {},
   "source": [
    "We can now test that this model manipulates a batch as expected and returns an estimate of flare intensity\n",
    "\n",
    "Note that the simple regression model definition requires knowing the number of channels and timesteps so here we pull that information from the configuration intializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0b6ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_channels: 13\n",
      "n_input_timestamps: 1\n",
      "input_dim: 13\n",
      "output_dim (spectrum length): 1343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RegressionEVEModel(\n",
       "  (linear): Linear(in_features=13, out_features=1343, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infer output dimension (number of wavelength bins) from the dataset label\n",
    "sample = train_dataset[0]\n",
    "output_dim = int(np.asarray(sample[\"forecast\"]).shape[0])\n",
    "\n",
    "# Input dimension should match the template convention: C * T (after spatial pooling)\n",
    "channel_order = config[\"data\"][\"channels\"]\n",
    "n_channels = len(channel_order)\n",
    "n_input_timestamps = config[\"model\"][\"time_embedding\"][\"time_dim\"]\n",
    "input_dim = n_channels * n_input_timestamps\n",
    "\n",
    "print(\"n_channels:\", n_channels)\n",
    "print(\"n_input_timestamps:\", n_input_timestamps)\n",
    "print(\"input_dim:\", input_dim)\n",
    "print(\"output_dim (spectrum length):\", output_dim)\n",
    "\n",
    "eve_model = RegressionEVEModel(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    channel_order=channel_order,\n",
    "    scalers=scalers,\n",
    ")\n",
    "\n",
    "eve_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee1e68",
   "metadata": {},
   "source": [
    "Now we can pass the input stack 'ts' to the model to transform it into our regression output.   Note that since this model has not been trained and was initalized randomly.  The output here has no real meaning.  It only acts as a test that our model forward doesn't have dimension problems.\n",
    "\n",
    "Dimension problemns are the dominant source of error in this kind of work.\n",
    "\n",
    "Note that our output has now the size of our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7d97841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1343]) torch.Size([2, 1343])\n",
      "target shape: (2, 1343)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_data_loader))\n",
    "\n",
    "eve_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = eve_model(batch)\n",
    "\n",
    "print(output.shape, batch[\"forecast\"].shape)\n",
    "print(\"target shape:\", tuple(batch[\"forecast\"].shape))\n",
    "\n",
    "# Expect: yhat = (B, output_dim), target = (B, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f32e64",
   "metadata": {},
   "source": [
    "## Define your metrics\n",
    "\n",
    "Metrics are a very important part of training AI models.   They provide your models with the quantitification of error, which in turn shifts the weights towards better pefrorming models.  They also provide a way for you to monitor performance, identify overfitting, and quantify value added. \n",
    "\n",
    "We now initialize the metrics class which allows you to control what metrics do you want to use as \"loss\" (i.e. the metrics that backpropagate through your model) and which ones for monitoring performance.  As with other components, this takes the form of a loaded module that can be later use in a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34c502ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.metrics.template_metrics import FlareMetrics\n",
    "from downstream_apps.template.metrics.eve_metrics import EVEMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd419b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EVEMetrics:\n",
    "    def __init__(self, mode: str, eps: float = 1e-12):\n",
    "        self.mode = mode\n",
    "        self.eps = eps\n",
    "\n",
    "    def _standardize(self, preds: torch.Tensor, target: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        preds = preds.float()\n",
    "        target = target.float()\n",
    "\n",
    "        # If preds/target come in with trailing singleton dims (e.g., (B, L, 1)), squeeze them.\n",
    "        while preds.ndim > 2 and preds.shape[-1] == 1:\n",
    "            preds = preds.squeeze(-1)\n",
    "        while target.ndim > 2 and target.shape[-1] == 1:\n",
    "            target = target.squeeze(-1)\n",
    "\n",
    "        # Ensure at least 2D for consistent downstream math\n",
    "        if preds.ndim == 1:\n",
    "            preds = preds.unsqueeze(-1)\n",
    "        if target.ndim == 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "\n",
    "        if preds.shape != target.shape:\n",
    "            raise ValueError(f\"Shape mismatch: preds {tuple(preds.shape)} vs target {tuple(target.shape)}\")\n",
    "\n",
    "        return preds, target\n",
    "\n",
    "    def _rrse(self, preds: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RRSE = sqrt( sum((pred-target)^2) / sum((target-mean(target))^2) )\n",
    "        Computed over all elements (batch and wavelength), returns scalar tensor [].\n",
    "        \"\"\"\n",
    "        diff2 = (preds - target) ** 2\n",
    "        num = diff2.sum()\n",
    "\n",
    "        mu = target.mean()\n",
    "        den = ((target - mu) ** 2).sum() + self.eps\n",
    "\n",
    "        return torch.sqrt(num / den)\n",
    "\n",
    "    def train_loss(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds, target = self._standardize(preds, target)\n",
    "        output_metrics = {}\n",
    "        output_weights = []\n",
    "\n",
    "        output_metrics[\"mse\"] = torch.nn.functional.mse_loss(preds, target)\n",
    "        output_weights.append(1)\n",
    "\n",
    "        return output_metrics, output_weights\n",
    "\n",
    "    def train_metrics(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds, target = self._standardize(preds, target)\n",
    "        output_metrics = {}\n",
    "        output_weights = []\n",
    "\n",
    "        output_metrics[\"rrse\"] = self._rrse(preds, target)\n",
    "        output_weights.append(1)\n",
    "\n",
    "        return output_metrics, output_weights\n",
    "\n",
    "    def val_metrics(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds, target = self._standardize(preds, target)\n",
    "        output_metrics = {}\n",
    "        output_weights = []\n",
    "\n",
    "        output_metrics[\"mse\"] = torch.nn.functional.mse_loss(preds, target)\n",
    "        output_weights.append(1)\n",
    "\n",
    "        output_metrics[\"rrse\"] = self._rrse(preds, target)\n",
    "        output_weights.append(1)\n",
    "\n",
    "        return output_metrics, output_weights\n",
    "\n",
    "    def __call__(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        match self.mode.lower():\n",
    "            case \"train_loss\":\n",
    "                return self.train_loss(preds, target)\n",
    "            case \"train_metrics\":\n",
    "                with torch.no_grad():\n",
    "                    return self.train_metrics(preds, target)\n",
    "            case \"val_metrics\":\n",
    "                with torch.no_grad():\n",
    "                    return self.val_metrics(preds, target)\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"{self.mode} is not implemented as a valid metric case.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b72d7d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, mode: str, eps: float = 1e-12)\n",
      "__main__\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(EVEMetrics.__init__))\n",
    "print(EVEMetrics.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d152b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the EVE metrics class you created (EVEMetrics) instead of FlareMetrics\n",
    "train_loss_metrics = EVEMetrics(\"train_loss\")\n",
    "train_evaluation_metrics = EVEMetrics(\"train_metrics\")\n",
    "validation_evaluation_metrics = EVEMetrics(\"val_metrics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550062c",
   "metadata": {},
   "source": [
    "Now they can be evaluated in our model's output and our ground truth.   First the loss that actually will backpropagate, in this case Mean Squared Errror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2baf9d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mse': tensor(1.6529)}, [1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_metrics(output, batch[\"forecast\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5eb6d",
   "metadata": {},
   "source": [
    "Then a training evaluation that will not backpropagate and inform our model, but that we can keep an eye on. Note that reporting lots of metrics during training will slow the training process.  I'm including it her as an example, but oftentimes is better to put the diagnostics only in the validation evaluation metrics.\n",
    "\n",
    "Here we are caclulating the Root Relative Squared Error https://lightning.ai/docs/torchmetrics/stable/regression/rse.html \n",
    "\n",
    "A value below one means the prediction is better than predicting the average.  It is unlikely that this metric will be lower than one with a randomly initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "796559f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rrse': tensor(1.4177)}, [1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_evaluation_metrics(output, batch[\"forecast\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65e7cd",
   "metadata": {},
   "source": [
    "In the validation evaluation metrics we report both MSE and RRSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a0bbb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mse': tensor(1.6529), 'rrse': tensor(1.4177)}, [1, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_evaluation_metrics(output, batch[\"forecast\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537871ed",
   "metadata": {},
   "source": [
    "## Define your PyTorch ligthning module\n",
    "\n",
    "In this workshop we will use PyTorch lightning to train our models.  PyTorch lighting reduces the amount of code required to implement a training loop in comparison to PyTorch (at the expense of control and versatility).  \n",
    "\n",
    "Opening the FlareLightningModule shows a simple Lightning model implementation.  It consists of:\n",
    "\n",
    "- An initialization of the class (metrics, model, and learning rate).\n",
    "- The forward code that runs evaluation of the model.\n",
    "- Training and validation steps.\n",
    "- Configuration of optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d730cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.lightning_modules.pl_simple_baseline import FlareLightningModule\n",
    "from downstream_apps.template.lightning_modules.eve_simple_baseline import EVELightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321963d",
   "metadata": {},
   "source": [
    "## Set your global seeds\n",
    "\n",
    "Since training AI models generally uses stochastic gradient descent, it is a good idea to fix your random seeds so that your training exercise is reproducible.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc28903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ec413",
   "metadata": {},
   "source": [
    "## Intialize Lightning module\n",
    "\n",
    "Now we properly initalize the Lightning module to enable training, including passing the dictionary of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553565e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'train_loss': train_loss_metrics,\n",
    "           'train_metrics': train_evaluation_metrics,\n",
    "           'val_metrics': validation_evaluation_metrics}\n",
    "\n",
    "learning_rate = 1e-3\n",
    "lit_model = EVELightningModule(model, metrics, lr=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c76c8",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "In order to properly compare experiments against each other, it is very useful to log evaluation metrics in a place where they can be compared against other training runs.  In this workshop we will use Weights and Biases (WandB). \n",
    "\n",
    "The first time you run WandB in a machine it will ask you to login to WandB.  You should have received an invitation to our project.  In order to login you must:\n",
    "\n",
    "- Select option 2 (existing account).   In VScode the dialog opens a box at the top of your screen.\n",
    "- Click on get API Key (this will open a browser).\n",
    "- Generate API Key.\n",
    "- Paste it in the dialog box at the top of your VSCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48c4d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"template_flare_regression\"\n",
    "run_name = \"baseline_experiment_1\"\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    entity=\"surya_handson\",\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    log_model=False,\n",
    "    save_dir=\"./wandb/wandb_tmp\",\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\"runs\", name=\"simple_flare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24e547",
   "metadata": {},
   "source": [
    "## Initialize trainer\n",
    "\n",
    "With the loggers done, now the trainer needs to be defined.  The trainer defines several properties of your training run. Here we define:\n",
    "\n",
    "- The max number of epochs (one epoch represents your model seeing your entire training dataset).\n",
    "- Define where the training run will take place (auto uses the GPU if possible, if not, CPU).\n",
    "- The loggers.\n",
    "- The callbacks (here we save the model with the lowest validation loss).\n",
    "- Logging frequency (because we are working with a small dataset it needs to be small)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b008c604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 2\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Trainer\n",
    "# -------------------------------------------------------------------------\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=[wandb_logger, csv_logger],\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "        )\n",
    "    ],\n",
    "    log_every_n_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088d108",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Finally we fit the model.  We pass the Lighting module, and our dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8a6e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/shahbahauddin/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshah-bahauddin\u001b[0m (\u001b[33mshah-bahauddin-university-of-colorado-boulder\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb/wandb_tmp/wandb/run-20260114_171155-zmx1rjni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/surya_handson/template_flare_regression/runs/zmx1rjni' target=\"_blank\">baseline_experiment_1</a></strong> to <a href='https://wandb.ai/surya_handson/template_flare_regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/surya_handson/template_flare_regression' target=\"_blank\">https://wandb.ai/surya_handson/template_flare_regression</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/surya_handson/template_flare_regression/runs/zmx1rjni' target=\"_blank\">https://wandb.ai/surya_handson/template_flare_regression/runs/zmx1rjni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "\n",
      "  | Name  | Type                 | Params | Mode  | FLOPs\n",
      "---------------------------------------------------------------\n",
      "0 | model | RegressionFlareModel | 14     | train | 0    \n",
      "---------------------------------------------------------------\n",
      "14        Trainable params\n",
      "0         Non-trainable params\n",
      "14        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b3ea1ed2654b1f914cfc7a8d619e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0e2c40e83b4ad4a35777e574af4fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9f582d345d4a7ca6b29b82b2ad80c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12612286b51647c78a9d62fe0dac884c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_model, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc1a23",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With this we have now integrated our dataset, dataloaders, metrics, and baseline into an end-2-end training loop.  The next step is to substitute the simple model with Surya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2628722",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surya_ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
