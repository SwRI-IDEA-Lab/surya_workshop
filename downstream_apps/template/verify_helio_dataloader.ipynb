{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da42003e",
   "metadata": {},
   "source": [
    "# HelioNetCDFDataset AWS / Local DataLoader Smoke Test\n",
    "\n",
    "This notebook validates that:\n",
    "\n",
    "1. `HelioNetCDFDataset` (from `helio_updated.py`) can be imported and instantiated.\n",
    "2. A child dataset pattern (from `template_dataset.py`) can subclass the AWS wrapper (`helio_aws.py`).\n",
    "3. A PyTorch `DataLoader` returns the expected `(sample_dict, metadata)` where:\n",
    "   - `sample_dict` contains the HelioFM keys (`ts`, `time_delta_input`, `forecast`, `lead_time_delta`) and optionally latitude keys if enabled.\n",
    "   - `metadata` contains file/timestamp context.\n",
    "\n",
    "Because this environment cannot access your real S3 bucket, the notebook creates a **tiny local NetCDF toy dataset** and runs the same data path your training loop uses. In AWS, you will swap the index to `s3://...` URIs and the same class will stream from S3 (optionally with `simplecache`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcdc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a clean environment, ensure dependencies are available.\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"Xarray:\", xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc72fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helio_updated.py and helio_aws.py from local files.\n",
    "# This avoids depending on your repo packaging for the smoke test.\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "def import_from_path(module_name: str, path: str):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    assert spec and spec.loader\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "HELIO_UPDATED_PATH = \"/mnt/data/helio_updated.py\"\n",
    "HELIO_AWS_PATH = \"/mnt/data/helio_aws.py\"\n",
    "\n",
    "helio_updated = import_from_path(\"helio_updated\", HELIO_UPDATED_PATH)\n",
    "helio_aws = import_from_path(\"helio_aws\", HELIO_AWS_PATH)\n",
    "\n",
    "HelioNetCDFDataset = helio_updated.HelioNetCDFDataset\n",
    "HelioNetCDFDatasetAWS = helio_aws.HelioNetCDFDatasetAWS\n",
    "\n",
    "print(\"Imported:\", HelioNetCDFDataset, HelioNetCDFDatasetAWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small local NetCDF toy dataset.\n",
    "# We make 3 files at times: t0, t0+60m, t0+120m to satisfy the dataset's required_timesteps logic.\n",
    "\n",
    "toy_root = Path(\"/mnt/data/toy_nc\")\n",
    "toy_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "t0 = pd.Timestamp(\"2013-01-01 00:00:00\")\n",
    "timesteps = [t0 + pd.Timedelta(minutes=m) for m in (0, 60, 120)]\n",
    "\n",
    "channels = [\"ch1\", \"ch2\"]\n",
    "H, W = 8, 8\n",
    "\n",
    "paths = []\n",
    "for ts in timesteps:\n",
    "    arr1 = np.random.rand(H, W).astype(np.float32)\n",
    "    arr2 = np.random.rand(H, W).astype(np.float32)\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"ch1\": ((\"y\", \"x\"), arr1),\n",
    "            \"ch2\": ((\"y\", \"x\"), arr2),\n",
    "        }\n",
    "    )\n",
    "    fp = toy_root / f\"{ts.strftime('%Y%m%d_%H%M')}.nc\"\n",
    "    # Use h5netcdf engine to match your reader (engine='h5netcdf').\n",
    "    ds.to_netcdf(fp, engine=\"h5netcdf\")\n",
    "    paths.append(str(fp))\n",
    "\n",
    "paths[:3], len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CSV index in the same shape as create_csv_index.py outputs:\n",
    "# columns: timestep, path, present\n",
    "\n",
    "index_df = pd.DataFrame(\n",
    "    {\n",
    "        \"timestep\": timesteps,\n",
    "        \"path\": paths,\n",
    "        \"present\": [1, 1, 1],\n",
    "    }\n",
    ")\n",
    "\n",
    "index_csv = Path(\"/mnt/data/toy_index.csv\")\n",
    "index_df.to_csv(index_csv, index=False)\n",
    "index_csv, index_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a child dataset following the template pattern (template_dataset.py),\n",
    "# but minimal: it adds a dummy downstream label under key 'forecast' (or 'ds_label').\n",
    "#\n",
    "# In your real use, you would import HelioNetCDFDatasetAWS from your package:\n",
    "#   from Surya.surya.datasets.helio_aws import HelioNetCDFDatasetAWS\n",
    "\n",
    "class ToyChildDataset(HelioNetCDFDatasetAWS):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Dummy downstream label per sample index\n",
    "        self._labels = np.linspace(0, 1, num=len(self.valid_indices), dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        base_dict, metadata = super().__getitem__(idx)\n",
    "        # Add a downstream label key similar to template_dataset.py\n",
    "        base_dict[\"ds_label\"] = np.array(self._labels[idx]).astype(np.float32)\n",
    "        return base_dict, metadata\n",
    "\n",
    "print(\"ToyChildDataset defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef118aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dataset and dataloader.\n",
    "#\n",
    "# Settings chosen to require exactly 3 timesteps per sample:\n",
    "#   - n_input_timestamps = 1\n",
    "#   - time_delta_input_minutes = [0]\n",
    "#   - rollout_steps = 1  => needs 2 target steps (rollout_steps + 1)\n",
    "#   - time_delta_target_minutes = 60 => target at +60m and +120m\n",
    "#\n",
    "# Resulting shapes (for C=2, H=W=8, n_input=1, rollout_steps=1):\n",
    "#   ts:        (C, T=1, H, W)\n",
    "#   forecast:  (C, L=2, H, W)\n",
    "\n",
    "dataset = ToyChildDataset(\n",
    "    index_path=str(index_csv),\n",
    "    time_delta_input_minutes=[0],\n",
    "    time_delta_target_minutes=60,\n",
    "    n_input_timestamps=1,\n",
    "    rollout_steps=1,\n",
    "    scalers=None,\n",
    "    num_mask_aia_channels=0,\n",
    "    drop_hmi_probability=0.0,\n",
    "    use_latitude_in_learned_flow=False,   # keep latitudes disabled for this smoke test\n",
    "    channels=channels,\n",
    "    phase=\"val\",\n",
    "    # AWS/S3 knobs are accepted (not used here since we are local):\n",
    "    s3_use_simplecache=True,\n",
    "    s3_cache_dir=\"/tmp/helio_s3_cache\",\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"len(dataset):\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one batch and validate keys / shapes.\n",
    "\n",
    "batch = next(iter(loader))\n",
    "sample_dict, metadata = batch\n",
    "\n",
    "print(\"Sample keys:\", sorted(sample_dict.keys()))\n",
    "for k, v in sample_dict.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(f\"{k:15s} tensor shape={tuple(v.shape)} dtype={v.dtype}\")\n",
    "    else:\n",
    "        # DataLoader collates numpy scalars into arrays/objects depending on type\n",
    "        print(f\"{k:15s} type={type(v)}\")\n",
    "\n",
    "print(\"\\nMetadata type:\", type(metadata))\n",
    "# metadata is typically a dict of lists/strings; print a couple fields if present:\n",
    "if isinstance(metadata, dict):\n",
    "    print(\"Metadata keys:\", list(metadata.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions: expected dictionary contract.\n",
    "# NOTE: latitude keys are only present when use_latitude_in_learned_flow=True.\n",
    "\n",
    "expected_base_keys = {\"ts\", \"time_delta_input\", \"forecast\", \"lead_time_delta\"}\n",
    "assert expected_base_keys.issubset(sample_dict.keys()), f\"Missing expected keys: {expected_base_keys - set(sample_dict.keys())}\"\n",
    "\n",
    "# Child key\n",
    "assert \"ds_label\" in sample_dict, \"Missing child-added key 'ds_label'\"\n",
    "\n",
    "# Shape checks (batch_size=2)\n",
    "C = len(channels)\n",
    "assert sample_dict[\"ts\"].shape[1] == C, \"Expected channel dimension at axis=1 after batching (B, C, T, H, W)\"\n",
    "assert sample_dict[\"forecast\"].shape[1] == C, \"Expected channel dimension at axis=1 after batching (B, C, L, H, W)\"\n",
    "\n",
    "print(\"All assertions passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88315623",
   "metadata": {},
   "source": [
    "## Running against real S3 (AWS environment)\n",
    "\n",
    "In AWS (EC2/EKS), you would point `index_path` to your S3 index CSV (or a local copy of it) where each `path` is an S3 URI, for example:\n",
    "\n",
    "`s3://nasa-surya-bench/2013/01/20130130_1924.nc`\n",
    "\n",
    "Then instantiate with the same class:\n",
    "\n",
    "```python\n",
    "dataset = HelioNetCDFDatasetAWS(\n",
    "    index_path=\"surya_aws_s3_val.csv\",\n",
    "    time_delta_input_minutes=[0, 12, 24, 36, 48, 60],\n",
    "    time_delta_target_minutes=12,\n",
    "    n_input_timestamps=6,\n",
    "    rollout_steps=4,\n",
    "    scalers=scalers,\n",
    "    channels=[...],\n",
    "    phase=\"val\",\n",
    "    s3_use_simplecache=True,\n",
    "    s3_cache_dir=\"/tmp/helio_s3_cache\",  # ideally fast local disk\n",
    ")\n",
    "```\n",
    "\n",
    "Operational guidance:\n",
    "- Run in the same AWS region as the bucket.\n",
    "- Prefer instance roles (IAM) over static keys.\n",
    "- Keep `s3_use_simplecache=True` unless you have a strong reason to disable it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
