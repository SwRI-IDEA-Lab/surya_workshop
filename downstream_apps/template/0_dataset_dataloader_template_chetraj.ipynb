{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d651fb69",
   "metadata": {},
   "source": [
    "# PyTorch dataset template\n",
    "\n",
    "by AndrÃ©s MuÃ±oz-Jaramillo\n",
    "\n",
    "This notebook is meant to act as a template to create a custom dataset based on a downstream application (DS) index.\n",
    "\n",
    "It requires an DS index file to be combined with a HelioFM index.  It also shows how to create a child database class based on HelioFM's database class so that all the code related to the input data is handled transparently, while the new code focuses exclusively in adding the DS information\n",
    "\n",
    "This template uses a flare forecasting dataset as an example, casting the problem as an X-ray flux regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed04321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import sunpy.visualization.colormaps as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Append base path.  May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to the wokshop_infrastructure folder.\n",
    "sys.path.append(\"../../\")\n",
    " \n",
    "# Append Surya path. May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to surya's release code.\n",
    "sys.path.append(\"../../Surya\")\n",
    "\n",
    "from surya.utils.data import build_scalers  # Data scaling utilities for Surya stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e9110",
   "metadata": {},
   "source": [
    "## Download scalers\n",
    "Surya input data needs to be scaled properly for the model to work and this cell downloads the scaling information.\n",
    "\n",
    "- If the cell below fails, try running the provided shell script directly in the terminal.\n",
    "- Sometimes the download may fail due to network or server issuesâ€”if that happens, simply re-run the script a few times until it completes successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b87776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Checking assets directory at: /home/jinsuhong/surya_workshop/downstream_apps/template/assets\n",
      "==> Downloading scalers.yaml from nasa-ibm-ai4science/core-sdo\n",
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 863.38it/s]\n",
      "/home/jinsuhong/surya_workshop/downstream_apps/template/assets\n",
      "âœ“ Done. File is located at: /home/jinsuhong/surya_workshop/downstream_apps/template/assets/scalers.yaml\n"
     ]
    }
   ],
   "source": [
    "!sh download_scalers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04135ff7",
   "metadata": {},
   "source": [
    "## Load configuration\n",
    "\n",
    "Surya was designed to read a configuration file that defines many aspects of the model\n",
    "including the data it uses we use this config file to set default values that do not\n",
    "need to be modified, but also to define values specific to our downstream application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c1fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Loading configuration...\n",
      "âœ… Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration paths - modify these if your files are in different locations\n",
    "config_path = \"./configs/config.yaml\"\n",
    "\n",
    "# Load configuration\n",
    "print(\"ðŸ“‹ Loading configuration...\")\n",
    "try:\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
    "    print(\"âœ… Configuration loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Make sure config.yaml exists in your current directory\")\n",
    "    raise\n",
    "\n",
    "scalers = build_scalers(info=config[\"data\"][\"scalers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6574dc",
   "metadata": {},
   "source": [
    "## Define DS dataset\n",
    "\n",
    "This child class takes as input all expected HelioFM parameters, plus additonal parameters relevant to the downstream application.  Here we focus in particular to the DS index and parameters necessary to combine it with the HelioFM index.\n",
    "\n",
    "Another important component of creating a dataset class for your DS is normalization.  Here we use a log normalization on xray flux that will act as the output target.  Making log10(xray_flux) strictly positive and having 66% of its values between 0 and 1\n",
    "\n",
    "Since we are going to use this dataset moving forward, it is better to develop it as script and not as a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34db2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.datasets.template_dataset import FlareDSDataset\n",
    "from downstream_apps.template.datasets.dataset_flare import SolarFlareDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319a7e4",
   "metadata": {},
   "source": [
    "## Initialize class without Surya stacks\n",
    "\n",
    "All the parameters that define a HelioFM dataset are contained within the test config file.  Scalers used to normalize HelioFm's input data are also necessary\n",
    "\n",
    "**_Important: This first initalization is set not to return the full Surya stack so that we can verify that the target is returning what we expect. This is so that you can check things quickly without having to pull full stacks until you need them._**\n",
    "\n",
    "_We do this by setting return_surya_stack=False_\n",
    "\n",
    "Make sure to set return_surya_stack=True if you need the full surya stack at this stage, otherwise we'll reinitialize the dataset shortly.\n",
    "\n",
    "**_Important:  In this notebook we sets max_number_of_samples=6 to potentially avoid going through the whole dataset as we explore it.  Keep in mind this for the future in case the database seems smaller than you expect_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d156801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SolarFlareDataset(\n",
    "    #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "    index_path=config[\"data\"][\"train_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probablity\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    s3_use_simplecache = True,\n",
    "    s3_cache_dir= \"/tmp/helio_s3_cache\",    \n",
    "    #### Put your donwnstream (DS) specific parameters below this line\n",
    "    label_type=\"label_max\",\n",
    "    return_surya_stack=False,\n",
    "    max_number_of_samples=6,\n",
    "    flare_index_path=\"./data/train.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82762d57",
   "metadata": {},
   "source": [
    "## Test length and structure\n",
    "\n",
    "Now we can test that the database is properly initialized and returns what is expected. In th case of the template, there are 6294 flares that take place during the training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677d7c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8530a021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'flare_timestamp'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = train_dataset.__getitem__(0)\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09615b0",
   "metadata": {},
   "source": [
    "Note that the dataset returns a single item and it's always the same item according to the index used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d2287e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': np.int64(0), 'flare_timestamp': 1273712400000000000}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09cab5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': np.int64(0), 'flare_timestamp': 1273723200000000000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed680710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': np.int64(0), 'flare_timestamp': 1273723200000000000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719afb0d",
   "metadata": {},
   "source": [
    "## Define dataloader\n",
    "\n",
    "With a working dataset we can define a dataloader.  A dataloader is simply a wrapper around a dataset that includes a sampling strategy to turn your dataset into batches.   Once we request a batch, the dataloader will return a dictionary like the dataset, but data inside will have a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f7caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=5,\n",
    "                num_workers=8\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399c8553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'flare_timestamp'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dad7b0",
   "metadata": {},
   "source": [
    "Now the batch will have more than one item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b93f7a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 0, 0, 0, 0]),\n",
       " 'flare_timestamp': tensor([1273712400000000000, 1273716000000000000, 1273719600000000000,\n",
       "         1273723200000000000, 1273726800000000000])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bce301",
   "metadata": {},
   "source": [
    "Typically we set dataloaders to shufle the data so that the model sees data in different order during training.  This means that in general we don't want and don't expect the batch to return the same sequence of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3acdc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 0, 0, 0, 0]),\n",
       " 'flare_timestamp': tensor([1273723200000000000, 1273730400000000000, 1273716000000000000,\n",
       "         1273726800000000000, 1273712400000000000])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=5,\n",
    "                shuffle=True,\n",
    "            )\n",
    "\n",
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307aaa56",
   "metadata": {},
   "source": [
    "## Initialize class with Surya stacks\n",
    "\n",
    "Now we initalize the database to return full surya stacks to visualize them by setting _return_surya_stack=True_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3081b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SolarFlareDataset(\n",
    "    #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "    index_path=config[\"data\"][\"train_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probablity\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    s3_use_simplecache = True,\n",
    "    s3_cache_dir= \"/tmp/helio_s3_cache\",    \n",
    "    #### Put your donwnstream (DS) specific parameters below this line\n",
    "    label_type=\"label_max\",\n",
    "    return_surya_stack=True,\n",
    "    max_number_of_samples=6,\n",
    "    flare_index_path=\"./data/train.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70e9c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = FlareDSDataset(\n",
    "#     #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "#     index_path=config[\"data\"][\"train_data_path\"],\n",
    "#     time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "#     time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "#     n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "#     rollout_steps=config[\"rollout_steps\"],\n",
    "#     channels=config[\"data\"][\"channels\"],\n",
    "#     drop_hmi_probability=config[\"drop_hmi_probablity\"],\n",
    "#     use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "#     scalers=scalers,\n",
    "#     phase=\"train\",\n",
    "#     s3_use_simplecache = True,\n",
    "#     s3_cache_dir= \"/tmp/helio_s3_cache\",    \n",
    "#     #### Put your donwnstream (DS) specific parameters below this line\n",
    "#     return_surya_stack=True,\n",
    "#     max_number_of_samples=6,\n",
    "#     ds_flare_index_path=\"./data/hek_flare_catalog.csv\",\n",
    "#     ds_time_column=\"start_time\",\n",
    "#     ds_time_tolerance = \"4d\",\n",
    "#     ds_match_direction = \"forward\"    \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18403f11",
   "metadata": {},
   "source": [
    "In surya's convention an item contains the following elements:\n",
    "- 'ts': input tensor.\n",
    "- 'time_delta_input': minutes with respect to the present in the time dimension of the input tensor.\n",
    "- 'forecast': target SDO stack.\n",
    "- 'lead_time_delta': how many minutes into the future is the target stack with respect to the present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9108229",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/tmp/helio_s3_cache/9e858e2ea1d4b43a2a1f40d8f19fd806ee62f92403e187e9ed4703bdb7918271'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m item\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/surya_workshop/downstream_apps/template/../../downstream_apps/template/datasets/dataset_flare.py:111\u001b[0m, in \u001b[0;36mSolarFlareDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    108\u001b[0m base_dictionary \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_surya_stack:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# This lines assembles the dictionary that Surya's dataset returns (defined above)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     base_dictionary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m reference_timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_indices[idx]\n\u001b[1;32m    114\u001b[0m base_dictionary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflare_index\u001b[38;5;241m.\u001b[39mloc[\n\u001b[1;32m    115\u001b[0m     reference_timestamp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_type\n\u001b[1;32m    116\u001b[0m ]\n",
      "File \u001b[0;32m~/surya_workshop/downstream_apps/template/../../workshop_infrastructure/datasets/helio.py:392\u001b[0m, in \u001b[0;36mHelioNetCDFDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    390\u001b[0m exception_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_counter \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_exception:\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    394\u001b[0m reference_timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_indices[idx]\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed retrieving index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Timestamp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference_timestep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m )\n",
      "File \u001b[0;32m~/surya_workshop/downstream_apps/template/../../workshop_infrastructure/datasets/helio.py:388\u001b[0m, in \u001b[0;36mHelioNetCDFDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_index_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    390\u001b[0m         exception_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/surya_workshop/downstream_apps/template/../../workshop_infrastructure/datasets/helio.py:429\u001b[0m, in \u001b[0;36mHelioNetCDFDataset._get_index_data\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    425\u001b[0m reference_timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_indices[idx]\n\u001b[1;32m    426\u001b[0m required_timesteps \u001b[38;5;241m=\u001b[39m reference_timestep \u001b[38;5;241m+\u001b[39m time_deltas\n\u001b[1;32m    428\u001b[0m sequence_data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_data(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_nc_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m required_timesteps\n\u001b[1;32m    431\u001b[0m ]\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Split sequence_data into inputs and target\u001b[39;00m\n\u001b[1;32m    434\u001b[0m inputs \u001b[38;5;241m=\u001b[39m sequence_data[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/surya_workshop/downstream_apps/template/../../workshop_infrastructure/datasets/helio.py:584\u001b[0m, in \u001b[0;36mHelioNetCDFDataset.load_nc_data\u001b[0;34m(self, filepath, timestep, channels)\u001b[0m\n\u001b[1;32m    581\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_s3fs()\n\u001b[1;32m    582\u001b[0m     opener \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mopen(filepath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 584\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopener\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# h5netcdf/h5py require a seekable file-like object; s3fs provides this\u001b[39;49;00m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# via ranged GET requests.\u001b[39;49;00m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh5netcdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/fsspec/core.py:105\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_magic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/fsspec/implementations/cached.py:476\u001b[0m, in \u001b[0;36mCachingFileSystem.__getattribute__.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;66;03m# all the methods defined in this class. Note `open` here, since\u001b[39;00m\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;66;03m# it calls `_open`, but is actually in superclass\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/fsspec/spec.py:1349\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1349\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1358\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/fsspec/implementations/cached.py:476\u001b[0m, in \u001b[0;36mCachingFileSystem.__getattribute__.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;66;03m# all the methods defined in this class. Note `open` here, since\u001b[39;00m\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;66;03m# it calls `_open`, but is actually in superclass\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/fsspec/implementations/cached.py:943\u001b[0m, in \u001b[0;36mSimpleCacheFileSystem._open\u001b[0;34m(self, path, mode, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m             f2\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(path, mode)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/s3fs/core.py:1400\u001b[0m, in \u001b[0;36mS3FileSystem._get_file\u001b[0;34m(self, rpath, lpath, callback, version_id, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m bytes_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f0:\n\u001b[1;32m   1401\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1402\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/tmp/helio_s3_cache/9e858e2ea1d4b43a2a1f40d8f19fd806ee62f92403e187e9ed4703bdb7918271'"
     ]
    }
   ],
   "source": [
    "item = train_dataset.__getitem__(0)\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba5cc8",
   "metadata": {},
   "source": [
    "The shape of the input tensors has dimensions [C, T, H, W], where\n",
    "- C: instrument channels.\n",
    "- T: timestamps.\n",
    "- H: Height.\n",
    "- W: Width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "item['ts'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0731f",
   "metadata": {},
   "source": [
    "## Plotting input stack\n",
    "\n",
    "Before plotting, it is necessary to undo the z-score and logarithmic normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_ts = train_dataset.inverse_transform_data(item['ts'][:,0,...])\n",
    "channel_order = config[\"data\"][\"channels\"]\n",
    "fig = plt.figure(figsize=np.array([4,4]), dpi=300)\n",
    "gs = gridspec.GridSpec(4, 4, figure=fig, wspace=0, hspace=0)\n",
    "\n",
    "limits = {}\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        n = i*4 + j\n",
    "        if n < len(channel_order):\n",
    "\n",
    "            ax = fig.add_subplot(gs[i,j])\n",
    "            channel = channel_order[n]\n",
    "            if 'hmi' not in channel:\n",
    "                lim = np.percentile(unnormalized_ts[n,...][unnormalized_ts[n,...]!=0], 99)\n",
    "                ax.imshow(unnormalized_ts[n,...], cmap=f'sdo{channel}', vmin=0, vmax=lim)\n",
    "                font_color = 'w'\n",
    "\n",
    "            else:\n",
    "                font_color = 'k'\n",
    "                if \"_v\" not in channel:\n",
    "                    lim = 1000\n",
    "                    ax.imshow(unnormalized_ts[n,...], cmap=f'hmimag', vmin = -lim, vmax=lim)\n",
    "                else:\n",
    "                    lim = 1000\n",
    "                    ax.imshow(unnormalized_ts[n,...], cmap=f'coolwarm', vmin = -lim, vmax=lim)                    \n",
    "\n",
    "            ax.text(0.01, 0.99, channel, transform=ax.transAxes, horizontalalignment='left', verticalalignment='top', color=font_color, fontsize=5)  \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13f88b",
   "metadata": {},
   "source": [
    "## Define dataloader\n",
    "\n",
    "Now the dataloader will return also a surya stack alongside our flaring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3716611",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92a13a",
   "metadata": {},
   "source": [
    "And now it will also have a batch dimension of 2 (batch dimensions are typically the leftmost dimension in a tensor's shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['ts'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56b26e",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Once this notebook runs successfully you have a dataset and dataloaders done and ready to train a DS application. The next step continues in 1_baseline_template.ipynb which will involve putting together a simple baseline including metrics and a training loop that can be used to compare with Surya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f76e5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surya_ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
