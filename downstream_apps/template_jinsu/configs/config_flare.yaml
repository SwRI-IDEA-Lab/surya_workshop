# job_id: large_scale
job_id: AR-segmentation_lora_r32_a64_d0.1
data:
  # Use this to point at the paths of HelioFM data indices
  sdo_data_root_path: /nobackupnfs1/sroy14/processed_data/Helio/nc
  train_data_path: ../../data/indices/surya_aws_s3_full_index.csv
  valid_data_path: ../../data/indices/surya_aws_s3_full_index.csv
  channels: ['aia94', 'aia131', 'aia171', 'aia193', 'aia211', 'aia304', 'aia335', 'aia1600', 'hmi_m', 'hmi_bx', 'hmi_by', 'hmi_bz', 'hmi_v']
  time_delta_input_minutes: [0]
  time_delta_target_minutes: +60
  # n_input_timestamps: 1 #Optional integer to randomly sample time_delta_input_minutes
  batch_size: 1
  num_data_workers: 24
  prefetch_factor: 2
  scalers_path: ./assets/scalers.yaml
  #### Put your donwnstream (DS) specific parameters below this line
  ar_index_train: ["./assets/surya-bench-ar-segmentation/train.csv"]
  ar_index_valid: ["./assets/surya-bench-ar-segmentation/validation.csv"]
  ar_index_test: ["./assets/surya-bench-ar-segmentation/test.csv"]
  downstream:
    train_index_path: ./data/train.csv
    val_index_path: ./data/val.csv
    test_index_path: ./data/test.csv
    label_type: label_max


model:
  model_type: spectformer # New model type for PEFT LoRA finetuning
  # Spectformer options
  img_size: 4096
  patch_size: 16
  in_channels: 13
  time_embedding:
    type: linear      # Options: linear, fourier, perceiver
    n_queries: null   # Integer for perceiver; otherwise null
    time_dim: 1       # Integer for linear and fourier; otherwise null
  unet_embed_dim: null
  unet_blocks: null
  unet_concat_activations: false # Whether to concatenate activations (UNet) or not (Autoencoder/bottleneck) in the decoder
  embed_dim: 1280
  depth: 10
  spectral_blocks: 2
  num_heads: 16
  mlp_ratio: 4.0
  rpe: false
  drop_rate: 0.0
  window_size: 2
  dp_rank: 4
  learned_flow: false
  epochs_to_learn_flow: 0 # Start by training flow model only before freezing
  init_weights: false
  checkpoint_layers: [0,1,2,3,4,5,6,7,8,9]
  ensemble: null
  finetune: true
  nglo: 1
  global_average_pooling: False
  global_max_pooling: False
  attention_pooling: False
  transformer_pooling: False
  global_class_token: True
  penultimate_linear_layer: True
  freeze_backbone: false  # Set to False for full model finetuning with LoRA
  dropout: 0.2
  # PEFT LoRA configuration
  use_lora: true
  lora_config:
    r: 8  # LoRA rank
    lora_alpha: 8  # LoRA alpha parameter
    target_modules: ['q_proj', 'v_proj', 'k_proj', 'out_proj', 'fc1', 'fc2']  # Target modules for LoRA
    lora_dropout: 0.1
    bias: 'none'
    # task_type: 'SEQ_CLS'

downstream_model:
  ckpt_path: ./assets/check_point
  ckpt_file: null
  resume: false
  load_weight_only: false
  in_channels: 52
  hidden_channels: [1280, 640, 1] # head: [1280, 640, 1] # simple_baseline: [52, 26, 1]
  threshold: 0.5
  dropout: 0.1

wandb:
  project_name: template_flare_classification
  run_name: simple_mlp_surya_backbone
  save_dir: ./wandb/wandb_tmp

trainer:
  accelerator: auto
  devices: auto
  precision: bf16-mixed
  log_every_n_steps: 2
  limit_train_batches: 50
  limit_val_batches: 20

  # metrics for AR segmentation
  select: "bce"
  dice:
    smooth: 1e-5
  iou:
    eps: 1e-6
  bce:
    pos_weight: 1

adapter:
  use_channel_adapter: False
  channels: ['aia94', 'aia131', 'aia171']

opt:
  optimizer_type: adamw
  epoch: 10
  lr: 0.0001
  weight_decay: 0.00001

scheduler:
  scheduler_type: onecyclelr
  anneal_strategy: cos

# optimizer:
#   warm_up_steps: 0 #2000
#   max_epochs: 15
#   learning_rate: 0.001
#   min_lr: 0.000001
use_latitude_in_learned_flow: false
loss_weights: [] # [] or list of weights per channel
rollout_steps: 0
num_mask_aia_channels: 0
drop_hmi_probablity: 0.0
validate_after_epoch: 1
wandb_log_train_after: 5 # This should be less than iters_per_epoch_train
wandb_project: "helio-fm" # Typical choices: "helio-fm", "spectformer_with_metrics"
visualization_samples: 3
save_wt_after_iter: 100000
path_experiment: checkpoints
iters_per_epoch_train: 2000
iters_per_epoch_valid: 200
iters_grad_accum: 1
dtype: bfloat16
parallelism: "ddp" # Valid options: "ddp" and "fsdp"
seed_num: 42

# Finetuning config fields
pretrained_path: ./assets/surya.366m.v1.pt
freeze_backbone: False
