{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d651fb69",
   "metadata": {},
   "source": [
    "# PyTorch Lightning fine-tuning template\n",
    "\n",
    "by AndrÃ©s MuÃ±oz-Jaramillo\n",
    "\n",
    "This notebook is meant to act as a template to train and use a surya model to implement DS application.\n",
    "\n",
    "It focuses on the concept of defining a modified Surya model, loading its weigths, and using a PyTorch lightning training loop to train it\n",
    "\n",
    "This notebook assumes familiarity with the concepts of datasets and dataloaders contained in the **_0_dataset_dataloader_template.ipynb_**\n",
    "\n",
    "It doesn't require having seen the baselines template, but they are meant to complement each other.  **_In fact they are on purpose almost identical!!!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bade9c",
   "metadata": {},
   "source": [
    "## Set your cuda visible device\n",
    "\n",
    "**IMPORTANT:** Since we are sharing resources, please make sure that the cuda visible device you put here is the one assigned to your team and your machine.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6af87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuan-winter\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"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\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080614c",
   "metadata": {},
   "source": [
    "Here we initalize variables related to Weights and Biases, our online logging system to ensure they are user specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca708db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure wandb logs are stored in a user-specific directory\n",
    "# Set writable directories\n",
    "os.environ[\"WANDB_DIR\"] = \"./wandb/wandb_logs\"\n",
    "os.environ[\"WANDB_CACHE_DIR\"] = \"./wandb/wandb_cache\"\n",
    "os.environ[\"WANDB_CONFIG_DIR\"] = \"./wandb/wandb_config\"\n",
    "# Optional:\n",
    "# os.environ[\"TMPDIR\"] = \"./wandb/wandb_tmp\"\n",
    "\n",
    "# Ensure directories exist (optional, wandb usually creates them)\n",
    "os.makedirs(os.environ[\"WANDB_DIR\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"WANDB_CACHE_DIR\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"WANDB_CONFIG_DIR\"], exist_ok=True)\n",
    "# os.makedirs(os.environ[\"TMPDIR\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed04321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger, WandbLogger\n",
    "\n",
    "# Append base path.  May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to the wokshop_infrastructure folder.\n",
    "sys.path.append(\"../../\")\n",
    " \n",
    "# Append Surya path. May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to surya's release code.\n",
    "sys.path.append(\"../../Surya\")\n",
    "\n",
    "from surya.utils.data import build_scalers  # Data scaling utilities for Surya stacks\n",
    "from workshop_infrastructure.utils import apply_peft_lora\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e9110",
   "metadata": {},
   "source": [
    "## Download scalers and Weights\n",
    "Surya input data needs to be scaled properly for the model to work and this cell downloads the scaling information.  In this notebook we also download the model weights for finetuning\n",
    "\n",
    "\n",
    "- If the cell below fails, try running the provided shell script directly in the terminal.\n",
    "- Sometimes the download may fail due to network or server issuesâ€”if that happens, simply re-run the script a few times until it completes successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b87776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Checking assets directory at: /home/andonghu/workspace/surya_workshop/downstream_apps/andong/assets\n",
      "==> Downloading scalers and model weights into: /home/andonghu/workspace/surya_workshop/downstream_apps/andong/assets\n",
      "/home/andonghu/workspace/surya_workshop/downstream_apps/andong/assets\n",
      "/home/andonghu/workspace/surya_workshop/downstream_apps/andong/assets\n",
      "âœ“ Done. Files are in: /home/andonghu/workspace/surya_workshop/downstream_apps/andong/assets\n"
     ]
    }
   ],
   "source": [
    "!sh download_scalers_and_weights.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04135ff7",
   "metadata": {},
   "source": [
    "## Load configuration\n",
    "\n",
    "Surya was designed to read a configuration file that defines many aspects of the model\n",
    "including the data it uses we use this config file to set default values that do not\n",
    "need to be modified, but also to define values specific to our downstream application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c1fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Loading configuration...\n",
      "âœ… Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration paths - modify these if your files are in different locations\n",
    "config_path = \"./configs/config.yaml\"\n",
    "\n",
    "# Load configuration\n",
    "print(\"ðŸ“‹ Loading configuration...\")\n",
    "try:\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
    "    print(\"âœ… Configuration loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Make sure config.yaml exists in your current directory\")\n",
    "    raise\n",
    "\n",
    "scalers = build_scalers(info=config[\"data\"][\"scalers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6574dc",
   "metadata": {},
   "source": [
    "## Define Downstream (DS) datasets\n",
    "\n",
    "This child class takes as input all expected HelioFM parameters, plus additonal parameters relevant to the downstream application.  Here we focus in particular to the DS index and parameters necessary to combine it with the HelioFM index.\n",
    "\n",
    "Another important component of creating a dataset class for your DS is normalization.  Here we use a log normalization on xray flux that will act as the output target.  Making log10(xray_flux) strictly positive and having 66% of its values between 0 and 1\n",
    "\n",
    "In this case we will define both a training and a validation dataset using the indices pointed at in the config\n",
    "\n",
    "**_Important:  In this notebook we sets max_number_of_samples=6 to potentially avoid going through the whole dataset as we explore it.  Keep in mind this for the future in case the database seems smaller than you expect_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db2cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from downstream_apps.template.datasets.template_dataset import FlareDSDataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdownstream_apps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mandong\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_andong\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DstDataset\n\u001b[1;32m      5\u001b[0m dst_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Dst_data/ML_Ready_Dataset_2019-2026-3h.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/workspace/surya_workshop/downstream_apps/andong/../../downstream_apps/andong/datasets/dataset_andong.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mworkshop_infrastructure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelio_aws\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HelioNetCDFDatasetAWS\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDstDataset\u001b[39;00m(HelioNetCDFDatasetAWS):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# --- Parent Args ---\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# 1. Initialize Parent\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/surya_workshop/downstream_apps/andong/../../workshop_infrastructure/datasets/helio_aws.py:28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mworkshop_infrastructure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HelioNetCDFDataset\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mHelioNetCDFDatasetAWS\u001b[39;00m(HelioNetCDFDataset):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"AWS/S3-oriented wrapper for :class:`~infrastructure.datasets.helio.HelioNetCDFDataset`.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    This class is intended to be imported and subclassed by downstream datasets\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    (see template_dataset.py). It behaves identically to HelioNetCDFDataset, but\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m        Keyword args to s3fs.S3FileSystem (rarely needed when using IAM roles).\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/surya_workshop/downstream_apps/andong/../../workshop_infrastructure/datasets/helio.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m     fsspec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     s3fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m njit, prange\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhdf5plugin\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@njit\u001b[39m(parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfast_transform\u001b[39m(data, means, stds, sl_scale_factors, epsilons):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/numba/__init__.py:73\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m get_versions\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m generate_version_info\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types, errors\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Re-export typeof\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/numba/core/config.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     _HAVE_YAML \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllvmlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbinding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mll\u001b[39;00m\n\u001b[1;32m     20\u001b[0m IS_WIN32 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m IS_OSX \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/llvmlite/binding/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThings that rely on the LLVM library\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdylib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutionengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitfini\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/llvmlite/binding/executionengine.py:314\u001b[0m\n\u001b[1;32m    308\u001b[0m _ObjectCacheGetBufferFunc \u001b[38;5;241m=\u001b[39m CFUNCTYPE(\u001b[38;5;28;01mNone\u001b[39;00m, py_object,\n\u001b[1;32m    309\u001b[0m                                       POINTER(_ObjectCacheData))\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# XXX The ctypes function wrappers are created at the top-level, otherwise\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# there are issues when creating CFUNCTYPEs in child processes on CentOS 5\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# 32 bits.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m _notify_c_hook \u001b[38;5;241m=\u001b[39m \u001b[43m_ObjectCacheNotifyFunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mExecutionEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_object_cache_notify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m _getbuffer_c_hook \u001b[38;5;241m=\u001b[39m _ObjectCacheGetBufferFunc(\n\u001b[1;32m    317\u001b[0m     ExecutionEngine\u001b[38;5;241m.\u001b[39m_raw_object_cache_getbuffer)\n\u001b[1;32m    319\u001b[0m ffi\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mLLVMPY_CreateObjectCache\u001b[38;5;241m.\u001b[39margtypes \u001b[38;5;241m=\u001b[39m [_ObjectCacheNotifyFunc,\n\u001b[1;32m    320\u001b[0m                                              _ObjectCacheGetBufferFunc,\n\u001b[1;32m    321\u001b[0m                                              py_object]\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from downstream_apps.template.datasets.template_dataset import FlareDSDataset\n",
    "\n",
    "from downstream_apps.andong.datasets.dataset_andong import DstDataset\n",
    "\n",
    "dst_data_path = \"../../Dst_data/ML_Ready_Dataset_2019-2026-3h.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9c5b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DstDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m STORM_LIMIT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m300.0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDstDataset\u001b[49m(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#### Standard Surya Parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     index_path\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data_path\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     time_delta_input_minutes\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_delta_input_minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     time_delta_target_minutes\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_delta_target_minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     n_input_timestamps\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m     rollout_steps\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrollout_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m     channels\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m     drop_hmi_probability\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_hmi_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m     use_latitude_in_learned_flow\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_latitude_in_learned_flow\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m     scalers\u001b[38;5;241m=\u001b[39mscalers,\n\u001b[1;32m     14\u001b[0m     phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     s3_use_simplecache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     s3_cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/helio_s3_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#### Dst Specific Parameters\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     return_surya_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m     dst_hdf5_path\u001b[38;5;241m=\u001b[39mdst_data_path,\n\u001b[1;32m     21\u001b[0m     delay_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Matches your \"3 day forecast\" requirement\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     max_number_of_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# Set to None to use full dataset\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#### NEW: Filter for storms\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     storm_threshold\u001b[38;5;241m=\u001b[39mSTORM_LIMIT\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m DstDataset(\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#### Standard Surya Parameters\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     index_path\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_data_path\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     storm_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     51\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DstDataset' is not defined"
     ]
    }
   ],
   "source": [
    "STORM_LIMIT = -300.0\n",
    "\n",
    "train_dataset = DstDataset(\n",
    "    #### Standard Surya Parameters\n",
    "    index_path=config[\"data\"][\"train_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probability\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    s3_use_simplecache=True,\n",
    "    s3_cache_dir=\"/tmp/helio_s3_cache\",\n",
    "    \n",
    "    #### Dst Specific Parameters\n",
    "    return_surya_stack=True,\n",
    "    dst_hdf5_path=dst_data_path,\n",
    "    delay_days=3,  # Matches your \"3 day forecast\" requirement\n",
    "    max_number_of_samples=None, # Set to None to use full dataset\n",
    "\n",
    "    #### NEW: Filter for storms\n",
    "    storm_threshold=STORM_LIMIT\n",
    ")\n",
    "\n",
    "val_dataset = DstDataset(\n",
    "    #### Standard Surya Parameters\n",
    "    index_path=config[\"data\"][\"valid_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probability\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"val\",\n",
    "    s3_use_simplecache=True,\n",
    "    s3_cache_dir=\"/tmp/helio_s3_cache\",\n",
    "    \n",
    "    #### Dst Specific Parameters\n",
    "    return_surya_stack=True,\n",
    "    dst_hdf5_path=dst_data_path,\n",
    "    delay_days=3,\n",
    "    max_number_of_samples=None,\n",
    "\n",
    "    #### NEW: Filter for storms\n",
    "    storm_threshold=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fda7cf",
   "metadata": {},
   "source": [
    "We also intialize separate training and validation dataloaders.   Since we are working in a shared environment.  Using multiprocessing_context=\"spawn\" helps avoid lockups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=16,\n",
    "                multiprocessing_context=\"spawn\",\n",
    "                persistent_workers=True,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "                dataset=val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=16,\n",
    "                multiprocessing_context=\"spawn\",\n",
    "                persistent_workers=True,\n",
    "                pin_memory=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d2dea",
   "metadata": {},
   "source": [
    "## Initialize the HelioSpectformer model\n",
    "\n",
    "This is the main difference beteween the notebook that trains the simple model and the one that fine-tunes Surya.  \n",
    "\n",
    "In the case of the finetuning exercise one of the main differences between DS applications is the dimensionality of the output.  In this notebook we use a modified HelioSpectformer that projects into a 1D space. \n",
    "\n",
    "**_IMPORTANT: If your DS application is 2D you need to use the HelioSpectformer2D_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fd639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from workshop_infrastructure.models.finetune_models import HelioSpectformer1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2fea80",
   "metadata": {},
   "source": [
    "Now the config file really comes into bear. The Spectformer has a metric ton of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1941589",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 216\n",
    "\n",
    "model = HelioSpectformer1D(\n",
    "    img_size=config[\"model\"][\"img_size\"],\n",
    "    patch_size=config[\"model\"][\"patch_size\"],\n",
    "    in_chans=config[\"model\"][\"in_channels\"],\n",
    "    embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "    time_embedding=config[\"model\"][\"time_embedding\"],\n",
    "    depth=config[\"model\"][\"depth\"],\n",
    "    num_heads=config[\"model\"][\"num_heads\"],\n",
    "    mlp_ratio=config[\"model\"][\"mlp_ratio\"],\n",
    "    drop_rate=config[\"model\"][\"drop_rate\"],\n",
    "    dtype=config[\"dtype\"],\n",
    "    window_size=config[\"model\"][\"window_size\"],\n",
    "    dp_rank=config[\"model\"][\"dp_rank\"],\n",
    "    learned_flow=config[\"model\"][\"learned_flow\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    init_weights=config[\"model\"][\"init_weights\"],\n",
    "    checkpoint_layers=config[\"model\"][\"checkpoint_layers\"],\n",
    "    n_spectral_blocks=config[\"model\"][\"spectral_blocks\"],\n",
    "    rpe=config[\"model\"][\"rpe\"],\n",
    "    ensemble=config[\"model\"][\"ensemble\"],\n",
    "    finetune=config[\"model\"][\"finetune\"],\n",
    "    nglo=config[\"model\"][\"nglo\"],\n",
    "\n",
    "    # Put finetuning additions below this line\n",
    "    dropout=config[\"model\"][\"dropout\"],\n",
    "    num_penultimate_transformer_layers=0,\n",
    "    num_penultimate_heads=0,\n",
    "    \n",
    "    num_outputs=output_dim,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7d0ee",
   "metadata": {},
   "source": [
    "## Load model weights\n",
    "\n",
    "Here we load the pre-trained checkpoint and load the weights.  The exercise of loading follows the idea of us as many of the weights as possible.  This is accomplished through the filtered_checkpoint_state.   It checks to see if the pretrained model's layers match those of your finetuning architecture.   It also checks that all your dimensions across layers check out.   If something does not work those paramameters are left in their random initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfff48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state = model.state_dict()\n",
    "checkpoint_state = torch.load(config[\"pretrained_path\"], weights_only=True, map_location=\"cpu\")\n",
    "filtered_checkpoint_state = {\n",
    "    k: v\n",
    "    for k, v in checkpoint_state.items()\n",
    "    if k in model_state and v.shape == model_state[k].shape\n",
    "}\n",
    "\n",
    "# 2. Load the filtered weights\n",
    "model_state.update(filtered_checkpoint_state)\n",
    "model.load_state_dict(model_state, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb6638",
   "metadata": {},
   "source": [
    "## To LoRA or not to Lora\n",
    "\n",
    "This cell gives you two options.  On the one hand we have the classic freezing of the backbone (the initial layers of the model).   On the other hand we have the use of a LoRA.\n",
    "\n",
    "LoRas have been a remarkable addition to our arsenal of models.   They have the advantage of keeping pretty much the entire model intact and only add broad modifications to weights as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a4319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PEFT LoRA with configuration: {'r': 8, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj', 'out_proj', 'fc1', 'fc2'], 'lora_dropout': 0.1, 'bias': 'none'}\n",
      "trainable params: 1,024,000 || all params: 360,608,728 || trainable%: 0.28%\n"
     ]
    }
   ],
   "source": [
    "use_LoRa = True\n",
    "\n",
    "if use_LoRa:\n",
    "    model = apply_peft_lora(model, config)\n",
    "else:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"embedding\" in name or \"backbone\" in name:\n",
    "            param.requires_grad = False\n",
    "    parameters_with_grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            parameters_with_grads.append(name)\n",
    "    print(\n",
    "        f\"{len(parameters_with_grads)} parameters require gradients: {', '.join(parameters_with_grads)}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee1e68",
   "metadata": {},
   "source": [
    "We can now test that this model manipulates a batch as expected and returns an estimate of flare intensity as we did for the simple baseline.\n",
    "\n",
    "We pass the input stack 'ts' to the model to transform it into our regression output.   Note that since this model was trained for a different task, it's likely it won't perform very well.  As with the simple baseline, this only acts as a test that our model forward doesn't have dimension problems.\n",
    "\n",
    "Dimension problemns are the dominant source of error in this kind of work.\n",
    "\n",
    "Note that our output has now the size of our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d97841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.6026e-01,  4.1012e-01, -2.8654e-01, -6.1035e-01,  7.5470e-01,\n",
       "          3.9172e-01, -4.8898e-01, -1.2010e-01, -5.0632e-01, -1.0640e+00,\n",
       "          6.4398e-01,  3.1738e-01,  5.7374e-01, -2.0757e-01,  2.3443e-01,\n",
       "          7.1691e-02, -2.7991e-01, -7.9097e-01, -1.9220e-01, -8.8596e-01,\n",
       "          2.2554e-01,  8.1129e-02, -1.1408e+00, -3.8075e-01, -1.3010e+00,\n",
       "          7.9412e-01,  3.6348e-01,  2.9376e-01,  2.4712e-01, -6.9403e-01,\n",
       "          8.0126e-01, -1.1842e-01,  5.0227e-01,  1.5094e-01,  4.7960e-01,\n",
       "         -5.3386e-01,  2.8118e-01, -7.5644e-01, -1.0135e+00,  1.7355e-01,\n",
       "          8.0459e-01,  4.0776e-01, -6.4233e-01, -1.1181e+00, -2.3914e-01,\n",
       "         -1.1957e+00,  6.6573e-01, -5.5135e-01,  6.7417e-01, -6.2748e-02,\n",
       "          8.9529e-01, -7.1778e-01, -1.1354e+00, -3.4343e-01,  6.7979e-01,\n",
       "         -3.0036e-02,  9.9391e-01,  6.2118e-01, -2.3254e-01, -2.8587e-01,\n",
       "          2.1151e-02, -1.0240e-01, -7.5554e-01,  1.4174e-01,  1.5326e-01,\n",
       "         -2.2378e-01,  5.1360e-01, -9.6082e-02, -1.2521e+00, -1.6647e-01,\n",
       "          7.2221e-01, -5.0671e-01,  3.8863e-01, -4.9268e-01,  3.9854e-01,\n",
       "          3.9889e-01,  3.1750e-01, -1.3783e-01, -3.5731e-02, -7.3974e-01,\n",
       "          6.9487e-01,  1.7801e-01, -9.5043e-01,  4.2542e-01,  2.8329e-01,\n",
       "         -6.0724e-02,  1.9557e-01, -1.5484e-01, -2.2287e-01, -4.4669e-01,\n",
       "          9.3801e-01, -3.4218e-01, -1.8236e-01,  9.4466e-01,  3.0822e-01,\n",
       "         -3.7935e-01, -6.7749e-01,  4.1767e-01, -3.1553e-01,  6.1186e-01,\n",
       "         -5.7008e-01,  2.3135e-01, -8.6129e-02,  4.9121e-01,  2.5133e-01,\n",
       "          8.2325e-02, -1.2168e+00, -1.4989e-01, -3.3522e-01,  3.9333e-01,\n",
       "         -2.5221e-01, -3.6189e-01, -4.3100e-01,  2.1400e-01,  7.7137e-01,\n",
       "         -1.4168e-01, -4.2986e-01,  1.0373e+00,  7.0850e-01,  1.4667e+00,\n",
       "          8.9909e-01,  1.8223e-01,  9.6588e-01, -8.1114e-01,  7.0585e-02,\n",
       "          6.2311e-01,  3.6669e-01,  4.7656e-01,  9.3180e-01, -5.3217e-01,\n",
       "         -5.2585e-01,  1.4772e-01,  4.3369e-01, -9.5340e-01,  2.3827e-01,\n",
       "          2.1749e-02, -1.1113e-01, -3.6109e-01,  2.1907e-01,  4.7770e-01,\n",
       "          1.3298e-01,  1.0168e-01, -6.9354e-01,  1.2796e-01,  3.1768e-01,\n",
       "         -7.5356e-01,  2.3275e-01, -8.7856e-01, -2.1114e-01,  1.1048e-01,\n",
       "          1.6800e-01,  4.4828e-01, -5.6534e-01, -6.0137e-01,  3.6217e-01,\n",
       "         -6.1191e-01,  3.4454e-01,  5.9758e-01, -4.9498e-01,  6.0876e-01,\n",
       "         -2.8579e-01,  4.4587e-01,  4.6285e-01,  1.2150e-01,  6.6103e-01,\n",
       "          1.3005e-01, -6.3284e-02,  4.1802e-01, -3.5333e-01, -4.4094e-01,\n",
       "         -1.7706e-02, -5.6527e-01, -1.5264e+00,  1.2604e-01,  1.6980e-01,\n",
       "         -7.2692e-02,  2.8169e-01, -2.9445e-01,  2.6058e-01,  8.9468e-01,\n",
       "          7.5524e-01, -5.8658e-01, -1.9949e-01,  2.9637e-01,  4.7956e-01,\n",
       "          1.8963e-01,  3.9451e-01, -7.9933e-01,  7.0454e-01,  3.8255e-01,\n",
       "          2.6456e-01,  4.2926e-01,  1.5813e-01, -5.8136e-01, -8.1551e-01,\n",
       "          6.2537e-01,  8.8780e-02,  3.3688e-01, -2.9778e-01,  2.2359e-01,\n",
       "          1.1337e+00, -4.0600e-01, -6.5918e-01,  7.1337e-01,  6.7935e-01,\n",
       "         -2.0866e-01,  2.5716e-01,  4.3174e-01,  2.7098e-01, -1.7890e-01,\n",
       "         -2.8167e-02, -2.7627e-01, -3.8793e-01,  1.3876e+00, -4.9693e-01,\n",
       "          1.0676e+00],\n",
       "        [ 5.8648e-01,  4.4332e-01, -8.8237e-01, -4.8077e-01,  4.2338e-02,\n",
       "          6.8356e-01, -2.9670e-01,  2.3376e-01, -1.0464e+00, -9.2614e-01,\n",
       "          2.3787e-01,  3.9588e-01,  5.1154e-01, -6.7745e-02,  5.7816e-01,\n",
       "         -3.0864e-01, -4.4658e-01, -5.3989e-01, -2.0135e-03, -7.9986e-01,\n",
       "         -1.3111e-01,  6.0939e-01, -1.5897e+00, -4.0447e-01, -7.2243e-01,\n",
       "          1.5629e-01, -3.3602e-01,  4.3356e-01,  5.2653e-01,  7.5217e-02,\n",
       "          1.1405e+00,  3.3024e-02,  4.7965e-02,  1.2422e-01,  6.4380e-01,\n",
       "         -3.4372e-01,  4.1783e-02, -9.5982e-01, -5.4417e-01,  1.8284e-03,\n",
       "          7.1836e-01,  5.9566e-01, -2.7012e-01, -5.8366e-01, -5.3626e-01,\n",
       "         -3.6725e-01,  5.8142e-01, -2.6520e-02,  1.2789e-01, -2.0366e-01,\n",
       "          3.0814e-01, -6.2441e-01, -1.8996e-01, -2.5105e-01,  5.7389e-02,\n",
       "          2.7341e-01,  5.6941e-01,  9.6644e-01,  1.0170e-01, -1.4903e-01,\n",
       "          2.0539e-01,  4.6999e-01, -2.1438e-01,  2.6172e-01,  1.9515e-01,\n",
       "         -4.2075e-01,  8.7611e-01, -1.9638e-02, -7.7142e-01,  1.9057e-01,\n",
       "          6.3358e-01, -3.2065e-01,  1.5796e-01, -9.7135e-02, -2.3470e-02,\n",
       "          2.1522e-01, -2.5506e-01, -3.8500e-02,  5.3720e-01, -4.9850e-01,\n",
       "          6.1924e-01, -1.7236e-01, -5.3825e-01,  3.6900e-01, -5.1344e-01,\n",
       "          7.7084e-02, -2.2143e-01, -3.3977e-01, -2.7798e-01, -4.8678e-01,\n",
       "          7.9012e-01, -6.7184e-01,  4.1840e-01,  1.2796e+00,  3.1684e-01,\n",
       "         -4.4759e-01, -1.0115e+00,  9.6977e-02, -3.7104e-01,  7.2965e-01,\n",
       "         -2.2010e-01,  1.9697e-01, -1.9162e-01,  4.0134e-01, -2.0170e-01,\n",
       "         -6.4866e-02, -1.2294e+00,  4.2510e-02, -3.7384e-01,  3.9661e-01,\n",
       "         -1.4136e-01, -2.9362e-01, -1.1993e+00,  3.2839e-01,  2.8406e-01,\n",
       "          2.6753e-02, -2.4477e-01,  9.9091e-01,  2.9123e-01,  1.8918e+00,\n",
       "         -1.2989e-01, -3.8700e-01,  9.9898e-01, -3.3048e-01, -4.9799e-01,\n",
       "          1.2762e-01,  3.5149e-01,  6.5783e-01,  4.8800e-01, -9.5226e-02,\n",
       "         -3.3898e-01, -2.6506e-01,  1.5219e-01, -4.8487e-01,  4.3703e-01,\n",
       "         -1.1293e-01, -1.6481e-01, -2.6299e-01,  1.0421e-01,  5.8207e-01,\n",
       "         -1.1074e-01,  1.1055e-01,  2.1748e-01, -1.3212e-02,  1.8881e-01,\n",
       "         -6.1507e-01,  1.6344e-01, -1.7437e-01, -2.5178e-01, -4.1755e-02,\n",
       "          5.8827e-01,  4.3105e-01, -2.5613e-01, -8.8913e-01,  2.9611e-01,\n",
       "          1.8358e-01, -2.5262e-01,  4.4558e-01, -2.5018e-01,  4.9052e-01,\n",
       "         -5.8517e-01,  7.5044e-01,  7.8314e-01,  6.0283e-02,  9.9296e-01,\n",
       "          1.4206e-01,  9.2299e-02, -5.6493e-02,  8.8406e-02, -1.0464e+00,\n",
       "          7.3931e-01, -1.5908e-01, -1.3100e+00,  5.3577e-01,  2.8406e-01,\n",
       "          3.0299e-01,  2.0945e-01, -4.0641e-01,  3.6037e-01,  9.8323e-01,\n",
       "          1.1209e+00, -4.9341e-03, -5.0796e-01,  5.0128e-01, -1.0822e-01,\n",
       "          3.6150e-01,  7.7449e-01, -6.8116e-01,  2.7619e-01,  5.0980e-01,\n",
       "          5.9676e-02,  3.0270e-01, -7.5078e-02, -6.8863e-01, -6.1184e-01,\n",
       "          8.8922e-02,  6.2354e-02,  9.1612e-03, -5.3477e-01,  7.2252e-01,\n",
       "          4.1088e-01, -8.9724e-01, -2.4083e-01,  3.8941e-01,  4.3700e-01,\n",
       "         -4.0875e-01,  3.1085e-01, -5.7015e-02, -3.7498e-01,  2.5264e-01,\n",
       "         -1.0174e-01, -3.5228e-01, -2.7594e-01,  1.2288e+00, -7.5647e-01,\n",
       "          1.1366e+00]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_data_loader))\n",
    "output = model.forward(batch)  # Get rid of singleton dimension\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f32e64",
   "metadata": {},
   "source": [
    "## Define your metrics\n",
    "\n",
    "Metrics are a very important part of training AI models.   They provide your models with the quantitification of error, which in turn shifts the weights towards better pefrorming models.  They also provide a way for you to monitor performance, identify overfitting, and quantify value added. \n",
    "\n",
    "We now initialize the metrics class which allows you to control what metrics do you want to use as \"loss\" (i.e. the metrics that backpropagate through your model) and which ones for monitoring performance.  As with other components, this takes the form of a loaded module that can be later use in a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c502ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.metrics.template_metrics import FlareMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_metrics = FlareMetrics(\"train_loss\")\n",
    "train_evaluation_metrics = FlareMetrics(\"train_metrics\")\n",
    "validation_evaluation_metrics = FlareMetrics(\"val_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550062c",
   "metadata": {},
   "source": [
    "Now they can be evaluated in our model's output and our ground truth.   First the loss that actually will backpropagate, in this case Mean Squared Errror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: torch.Size([2, 216])\n"
     ]
    }
   ],
   "source": [
    "print(\"Target shape:\", batch[\"forecast\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing metrics...\n"
     ]
    }
   ],
   "source": [
    "class DstMetrics:\n",
    "    def __init__(self, mode=\"train_loss\"):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __call__(self, preds, target):\n",
    "        # 1. Fix Target Shape [Batch, 3, 216] -> [Batch, 216]\n",
    "        if target.dim() == 3:\n",
    "            target = target[:, 0, :]\n",
    "            \n",
    "        # 2. Fix Preds Shape [Batch, 216, 1] -> [Batch, 216]\n",
    "        if preds.dim() == 3 and preds.shape[-1] == 1:\n",
    "            preds = preds.squeeze(-1)\n",
    "            \n",
    "        # 3. Calculate Loss\n",
    "        loss = torch.nn.functional.mse_loss(preds, target)\n",
    "        \n",
    "        # 4. Return (Dict, Weights) ALWAYS\n",
    "        if self.mode == \"train_loss\":\n",
    "            return {\"mse\": loss}, [1]\n",
    "        else:\n",
    "            # FIX: Return a dummy second value to prevent crash\n",
    "            return {\"mse\": loss}, [] \n",
    "\n",
    "# --- RE-INITIALIZE METRICS ---\n",
    "print(\"Re-initializing metrics...\")\n",
    "train_loss_metrics = DstMetrics(\"train_loss\")\n",
    "train_evaluation_metrics = DstMetrics(\"train_metrics\")\n",
    "validation_evaluation_metrics = DstMetrics(\"val_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5eb6d",
   "metadata": {},
   "source": [
    "Then a training evaluation that will not backpropagate and inform our model, but that we can keep an eye on. Note that reporting lots of metrics during training will slow the training process.  I'm including it her as an example, but oftentimes is better to put the diagnostics only in the validation evaluation metrics.\n",
    "\n",
    "Here we are caclulating the Root Relative Squared Error https://lightning.ai/docs/torchmetrics/stable/regression/rse.html \n",
    "\n",
    "A value below one means the prediction is better than predicting the average.  It is unlikely that this metric will be lower than one with a randomly initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796559f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mse': tensor(701.9774, grad_fn=<MseLossBackward0>)}, [])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_evaluation_metrics(output, batch[\"forecast\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65e7cd",
   "metadata": {},
   "source": [
    "In the validation evaluation metrics we report both MSE and RRSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0bbb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mse': tensor(701.9774, grad_fn=<MseLossBackward0>)}, [])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_evaluation_metrics(output, batch[\"forecast\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537871ed",
   "metadata": {},
   "source": [
    "## Define your PyTorch ligthning module\n",
    "\n",
    "In this workshop we will use PyTorch lightning to train our models.  PyTorch lighting reduces the amount of code required to implement a training loop in comparison to PyTorch (at the expense of control and versatility).  \n",
    "\n",
    "Opening the FlareLightningModule shows a simple Lightning model implementation.  It consists of:\n",
    "\n",
    "- An initialization of the class (metrics, model, and learning rate).\n",
    "- The forward code that runs evaluation of the model.\n",
    "- Training and validation steps.\n",
    "- Configuration of optimizers.\n",
    "\n",
    "**_Note that it is the same Lightning module we used for the baseline!!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d730cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.lightning_modules.pl_simple_baseline import FlareLightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321963d",
   "metadata": {},
   "source": [
    "## Set your global seeds\n",
    "\n",
    "Since training AI models generally uses stochastic gradient descent, it is a good idea to fix your random seeds so that your training exercise is reproducible.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ec413",
   "metadata": {},
   "source": [
    "## Intialize Lightning module\n",
    "\n",
    "Now we properly initalize the Lightning module to enable training, including passing the dictionary of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553565e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'train_loss': train_loss_metrics,\n",
    "           'train_metrics': train_evaluation_metrics,\n",
    "           'val_metrics': validation_evaluation_metrics}\n",
    "\n",
    "learning_rate = 1e-5\n",
    "lit_model = FlareLightningModule(model, metrics, lr=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c76c8",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "In order to properly compare experiments against each other, it is very useful to log evaluation metrics in a place where they can be compared against other training runs.  In this workshop we will use Weights and Biases (WandB). \n",
    "\n",
    "The first time you run WandB in a machine it will ask you to login to WandB.  You should have received an invitation to our project.  In order to login you must:\n",
    "\n",
    "- Select option 2 (existing account).   In VScode the dialog opens a box at the top of your screen.\n",
    "- Click on get API Key (this will open a browser).\n",
    "- Generate API Key.\n",
    "- Paste it in the dialog box at the top of your VSCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"surya_dst_forecast\"\n",
    "run_name = \"dst_finetune_3day_delay\"\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    entity=\"surya_handson\",\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    log_model=False,\n",
    "    save_dir=\"./wandb/wandb_tmp\",\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\"runs\", name=\"dst_forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24e547",
   "metadata": {},
   "source": [
    "## Initialize trainer\n",
    "\n",
    "With the loggers done, now the trainer needs to be defined.  The trainer defines several properties of your training run. Here we define:\n",
    "\n",
    "- The max number of epochs (one epoch represents your model seeing your entire training dataset).\n",
    "- Define where the training run will take place (auto uses the GPU if possible, if not, CPU).\n",
    "- The loggers.\n",
    "- The callbacks (here we save the model with the lowest validation loss).\n",
    "- Logging frequency (because we are working with a small dataset it needs to be small).\n",
    "\n",
    "\n",
    "**Note that in this notebook we also set a mixed precision to reduce the model's footprint in memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008c604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Trainer\n",
    "# -------------------------------------------------------------------------\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=\"bf16-mixed\", \n",
    "    logger=[wandb_logger, csv_logger],\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "        )\n",
    "    ],\n",
    "    log_every_n_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088d108",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Finally we fit the model.  We pass the Lighting module, and our dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb/wandb_tmp/wandb/run-20260113_215145-ijznxttd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/surya_handson/surya_dst_forecast/runs/ijznxttd' target=\"_blank\">dst_finetune_3day_delay</a></strong> to <a href='https://wandb.ai/surya_handson/surya_dst_forecast' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/surya_handson/surya_dst_forecast' target=\"_blank\">https://wandb.ai/surya_handson/surya_dst_forecast</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/surya_handson/surya_dst_forecast/runs/ijznxttd' target=\"_blank\">https://wandb.ai/surya_handson/surya_dst_forecast/runs/ijznxttd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/anaconda3/envs/surya_ws/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type      | Params | Mode  | FLOPs\n",
      "----------------------------------------------------\n",
      "0 | model | PeftModel | 360 M  | train | 0    \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "359 M     Non-trainable params\n",
      "360 M     Total params\n",
      "1,442.435 Total estimated model params size (MB)\n",
      "369       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf74d0cf08049a2bf45fb657f71f84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b06ee83b6547768fe703e0851f1e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lit_model, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc1a23",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With this we have now integrated our dataset, dataloaders, metrics, and DS into an end-2-end training loop and we are ready to experiment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surya_ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
